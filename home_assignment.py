# -*- coding: utf-8 -*-
"""Home assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cZw-cMefEcTt8oM85aknipDsPVS8LJk9
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.losses import MeanSquaredError, CategoricalCrossentropy
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
import os

# Task 1: Tensor Transformations & Reshaping
# Step 1: Generate a tensor with dimensions (4, 6)
tensor_data = tf.random.uniform((4, 6))
print("Generated Tensor:", tensor_data.numpy())

# Step 2: Determine tensor rank and shape
tensor_rank = tf.rank(tensor_data).numpy()
tensor_shape = tensor_data.shape
print(f"Rank: {tensor_rank}, Shape: {tensor_shape}")

# Step 3: Modify shape and transpose
reshaped_data = tf.reshape(tensor_data, (2, 3, 4))
transposed_data = tf.transpose(reshaped_data, perm=[1, 0, 2])
print("Reshaped Tensor:", reshaped_data.numpy())
print("Transposed Tensor:", transposed_data.numpy())

# Step 4: Broadcasting & Summation
small_data = tf.random.uniform((1, 4))
broadcasted_data = tf.broadcast_to(small_data, (4, 4))
result_data = tensor_data[:, :4] + broadcasted_data
print("Broadcasted Tensor:", broadcasted_data.numpy())
print("Summed Tensor:", result_data.numpy())

# Explanation of Broadcasting Concept
print("Broadcasting enables a smaller tensor to be extended to fit a larger tensor's shape for element-wise operations.")

# Task 2: Compute and Compare Loss Functions
y_actual = tf.constant([0.0, 1.0, 1.0, 0.0])
y_predicted = tf.constant([0.2, 0.9, 0.8, 0.1])

mse_loss_fn = MeanSquaredError()
cce_loss_fn = CategoricalCrossentropy()

mse_result = mse_loss_fn(y_actual, y_predicted).numpy()
cce_result = cce_loss_fn(tf.expand_dims(y_actual, axis=0), tf.expand_dims(y_predicted, axis=0)).numpy()
print(f"MSE Loss: {mse_result}, CCE Loss: {cce_result}")

# Alter predictions and compute loss again
y_predicted_updated = tf.constant([0.1, 0.8, 0.9, 0.2])
mse_updated = mse_loss_fn(y_actual, y_predicted_updated).numpy()
cce_updated = cce_loss_fn(tf.expand_dims(y_actual, axis=0), tf.expand_dims(y_predicted_updated, axis=0)).numpy()
print(f"Updated MSE Loss: {mse_updated}, Updated CCE Loss: {cce_updated}")

# Visualize Loss Functions
plt.bar(["MSE", "CCE"], [mse_result, cce_result], color=['blue', 'red'])
plt.xlabel("Loss Type")
plt.ylabel("Loss Value")
plt.title("MSE vs Cross-Entropy Loss Comparison")
plt.show()

# Task 3: Train Models Using Different Optimizers
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images, test_images = train_images / 255.0, test_images / 255.0

# Define a Simple Model
def build_model():
    model = Sequential([
        Flatten(input_shape=(28, 28)),
        Dense(128, activation='relu'),
        Dense(10, activation='softmax')
    ])
    return model

# Train with Adam Optimizer
adam_model = build_model()
adam_model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
adam_history = adam_model.fit(train_images, train_labels, epochs=5, validation_data=(test_images, test_labels))

# Train with SGD Optimizer
sgd_model = build_model()
sgd_model.compile(optimizer=SGD(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
sgd_history = sgd_model.fit(train_images, train_labels, epochs=5, validation_data=(test_images, test_labels))

# Plot and Compare Accuracy
plt.plot(adam_history.history['accuracy'], label='Adam')
plt.plot(sgd_history.history['accuracy'], label='SGD')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Optimizer Comparison: Adam vs SGD')
plt.legend()
plt.show()

# Task 4: Neural Network Training with TensorBoard
log_directory = "logs/fit/"
os.makedirs(log_directory, exist_ok=True)

model_tb = build_model()
model_tb.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_directory, histogram_freq=1)
model_tb.fit(train_images, train_labels, epochs=5, validation_data=(test_images, test_labels), callbacks=[tb_callback])

# Launch TensorBoard
print("To launch TensorBoard, use: tensorboard --logdir logs/fit/")

# Answering Key Questions
print("1. Observing accuracy curves: Adam converges quicker, whereas SGD takes longer but may generalize better.")
print("2. Detecting overfitting using TensorBoard: A widening gap between training and validation accuracy signals overfitting.")
print("3. Effect of increased epochs: More training can enhance accuracy initially but might lead to overfitting.")